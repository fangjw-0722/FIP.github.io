<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="FIP enables real-time, accurate motion capture using daily garments by fusing flex and inertial sensors.">
  <meta name="keywords" content="FIP, Motion Capture, Wearable Sensors, Flex Sensors, IMU">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>FIP: Robust Motion Capture on Daily Garments</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
</head>
<body>



<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
        <div class="columns is-centered">
            <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">FIP: Endowing Robust Motion Capture on Daily Garment by Fusing Flex and Inertial Sensors</h1>
            <div class="is-size-5 publication-authors">
                <span class="author-block">
                <a href="https://keunhong.com">Ruonan Zheng*</a><sup>1</sup>,
                <a href="https://utkarshsinha.com">Jiawei Fang*</a><sup>2</sup>
                </span>
            </div>
            <div class="is-size-5 publication-authors">
                <span class="author-block">
                <a href="https://jonbarron.info">Yuan Yao</a><sup>2</sup>,
                <a href="http://sofienbouaziz.com">Xiaoxia Gao</a><sup>2</sup>,
                <a href="https://www.danbgoldman.com">Chengxu Zuo</a><sup>2</sup>,
                <a href="https://homes.cs.washington.edu/~seitz/">Shihui Guo</a><sup>1,2</sup>,
                <a href="http://www.ricardomartinbrualla.com">Yiyue Luo</a><sup>2</sup>
                </span>
            </div>
            <p class="is-size-7 has-text-centered">* Equal contribution</p>
            <div class="column has-text-centered">
                <div class="publication-links">
                <span class="link-block">
                    <a href="./static/docs/FIP_paper.pdf" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-file-pdf"></i></span>
                    <span>Paper</span>
                    </a>
                </span>
                </div>
            </div>
            </div>
        </div>
        </div>
    </div>
    </section>


<section class="section">
    <div class="container is-max-desktop">
        <div class="has-text-centered">
        <img src="./static/images/Fig_teaser.jpg" alt="FIP Motion Capture Teaser" style="width:80%;">
        <p><em>FIP endows real-time, accurate motion capture on daily clothes by fusing flex and inertial sensors.</em></p>
        </div>
    </div>
    </section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            What if our clothes could capture our body motion accurately? This paper introduces Flexible Inertial Poser (FIP), a novel motion-capturing system using daily garments with two elbow-attached flex sensors and four Inertial Measurement Units (IMUs). However, the inevitable sensor displacements in loose wearables degrade joint tracking accuracy significantly. To address this, we identify the distinct characteristics of the flex and inertial sensor displacements and develop a Displacement Latent Diffusion Model and a Physics-informed Calibrator to compensate for sensor displacements based on such observations, resulting in a substantial improvement in motion capture accuracy. Notably, our system outperforms the accuracy of state-of-the-art (SOTA) real-time posture estimation with a significant advance in elbow joint tracking. FIP opens up opportunities for ubiquitous human-computer interactions and diverse interactive applications such as Metaverse, rehabilitation, and fitness analysis. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
    <h2 class="title is-3">Video</h2>
    <div class="publication-video has-text-centered">
      <iframe src="https://www.youtube.com/embed/YOUR_VIDEO_ID" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
    </div>
  </div>
</div>
</div>
</section>

<!-- fabrication -->


  
<section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Fabrication</h2>
          <div class="has-text-centered">
            <img src="./static/images/Fig_Prototyping.jpg" alt="The process of prototyping" style="width:80%;">
            <p><em>The process of prototyping: (a) Assemble the sensors together by soldering; (b) Cut the fabric into pieces according to the patterns; (c) Integrate the assembled sensors into the fabric through heat pressing and sew the fabric into the garment.</em></p>
          </div>
          <div class="content has-text-justified">
            <p>
              The fabrication of FIP garments follows a structured process integrating sensors seamlessly into daily wear.
              The system incorporates two flex sensors and four IMUs into a loose-fitting jacket, ensuring both comfort and accuracy.
              Sensors are embedded using heat pressing, and electrical components are routed through flexible wiring channels.
            </p>
            <p>
              The fabrication process consists of three stages: sensor assembly, fabric cutting, and garment integration.
              First, the flex sensors and IMUs are soldered onto their respective connection boards, forming the sensor network.
              Next, the garment is designed with specialized fabric patterns, ensuring proper placement of sensors.
              Finally, sensors are heat-pressed onto the fabric, with wiring seamlessly integrated to maintain garment flexibility and comfort.
            </p>
            <p>
              Our design prioritizes wearability and durability, making FIP suitable for real-world applications such as motion tracking in Metaverse, rehabilitation, and fitness monitoring.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

<!-- algorithm -->


<section class="section">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Algorithm</h2>
                <div class="content has-text-justified">
                    <p>
                        FIP's motion capture algorithm consists of three core modules:
                    </p>
                    <ul>
                        <li><strong>Displacement Latent Diffusion Model:</strong> This module synthesizes sensor displacement in various conditions, addressing real-time displacement of IMUs through a generative AI-based approach.</li>
                        <li><strong>Physics-informed Calibrator:</strong> A calibration mechanism that registers flex sensor data under different wearing conditions, compensating for primary displacement effects.</li>
                        <li><strong>Pose Fusion Predictor:</strong> A multi-modal fusion network that integrates readings from both IMUs and flex sensors to estimate accurate body postures.</li>
                    </ul>
                    <p>
                        These components work in synergy to enhance robustness, ensuring precise motion tracking even in loose-fitting garments.
                    </p>
                </div>
                <div class="has-text-centered">
                    <img src="./static/images/Fig_pipeline4.png" alt="Pipeline Overview" style="width:100%;">
                    <p><em>Pipeline Overview. Left: For data preparation, we first utilize a simulation body-fabric model to synthesize the IMU <strong>Real-time Displacement</strong>. Then, for training a robust pose predictor, we train a Displacement Latent Diffusion Model (DLDM) to generate enough diverse data that covers real-world distribution. At last, we train a Pose Fusion Predictor leveraging simulated flex sensor data and generated IMU data, with the supervision of SMPL Pose. Right: In our testing phase, flex sensor readings will be first input to our Physical-informed Calibrator to address the <strong>Primary Displacement</strong>, which will be input to the pre-trained Pose Fusion Predictor with IMU data.</em></p>
                </div>
            </div>
        </div>
    </div>
</section>




<section class="section">
<div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
        <h2 class="title is-3">Performance</h2>
        <div class="has-text-centered">
            <img src="./static/images/Fig_vis.jpg" alt="Qualitative results visualization" style="width:100%;">
            <p><em>Qualitative results: our approach outperforms all SOTA methods in motion capture with a clear advantage in elbow joint tracking.</em></p>
        </div>
        <div class="content has-text-justified">
        <p>
            Our FIP system achieves state-of-the-art motion capture performance by significantly reducing tracking errors. Key evaluation metrics include:
            </p>
            <ul>
            <li><strong>Angular Error:</strong> Reduced joint rotation error compared to existing real-time motion capture methods.</li>
            <li><strong>Elbow Tracking Accuracy:</strong> Improved tracking of elbow joints due to the fusion of flex and inertial sensors.</li>
            <li><strong>Positional Error:</strong> Lower deviation in joint positions, ensuring higher accuracy in full-body motion estimation.</li>
            <li><strong>Robustness:</strong> Effective compensation for sensor displacement, enabling stable tracking in loose-fitting garments.</li>
            </ul>
            <p>
            These results highlight FIP’s potential for applications in human-computer interaction, rehabilitation, and immersive virtual environments.
            </p>
        </div>
    </div>
    </div>
</div>
</section>


<section class="section">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
            <h2 class="title is-3">Application</h2>
            <div class="content has-text-justified">
                <p>
                  We demonstrate applications of our <em>Clothes-based MoCap</em> system in various human-computer interaction scenarios, including virtual and augmented reality (VR/AR), rehabilitation, and fitness analysis, leveraging the system's robustness, accessibility, and comfort.
                </p>
                <div class="has-text-centered">
                  <img src="./static/images/Fig_app.jpg" alt="Applications of our approach" style="width:100%;">
                  <p><em>Applications of our approach: (a) Metaverse, (b) rehabilitation, (c) fitness analysis.</em></p>
                </div>
        </div>
        </div>
    </div>
    </section>



<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>© 2025 FIP Research Team.</p>
    </div>
  </div>
</footer>

</body>
</html>
